"""
å­¦ä¹ é«˜æµé‡æ–‡ç« è„šæœ¬
é€šè¿‡æµè§ˆå™¨è‡ªåŠ¨åŒ–æœç´¢å°çº¢ä¹¦çƒ­é—¨ç¬”è®°ï¼ŒæŠ“å–å†…å®¹ä¸äº’åŠ¨æ•°æ®ï¼Œç”Ÿæˆåˆ†ææŠ¥å‘Š
"""

import argparse
import asyncio
import json
import re
from collections import Counter
from datetime import datetime
from pathlib import Path

from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table

import config
from browser_helper import launch_browser, close_browser, navigate_to, ensure_login, ensure_login_on_page
from utils import (
    random_delay, safe_click, extract_text, extract_attribute,
    parse_count, save_to_json, smooth_scroll, wait_for_any_selector, truncate_text,
)

console = Console()


def _load_selectors() -> dict:
    """åŠ è½½é€‰æ‹©å™¨é…ç½®"""
    with open(config.SELECTORS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


async def _apply_filters(page, sort: str = "hot"):
    """
    æ‰“å¼€ç­›é€‰é¢æ¿å¹¶é€‰æ‹©æ’åºæ–¹å¼å’Œå‘å¸ƒæ—¶é—´
    æ’åºä¾æ®: ç»¼åˆ / æœ€æ–° / æœ€å¤šç‚¹èµ / æœ€å¤šè¯„è®º / æœ€å¤šæ”¶è—
    å‘å¸ƒæ—¶é—´: ä¸é™ / ä¸€å¤©å†… / ä¸€å‘¨å†… / åŠå¹´å†…
    """
    # å†³å®šæ’åºæ–‡æœ¬
    sort_map = {
        "hot": "æœ€å¤šç‚¹èµ",
        "new": "æœ€æ–°",
        "comment": "æœ€å¤šè¯„è®º",
        "collect": "æœ€å¤šæ”¶è—",
    }
    sort_text = sort_map.get(sort, "ç»¼åˆ")
    date_text = "åŠå¹´å†…"  # é»˜è®¤ç­›é€‰åŠå¹´å†…çš„ç¬”è®°

    # ç‚¹å‡»ç­›é€‰æŒ‰é’®ï¼ˆå¿…é¡»ç”¨ JS clickï¼ŒPlaywright click ä¸è§¦å‘é¢æ¿ï¼‰
    filter_exists = await page.query_selector("div.filter")
    if not filter_exists:
        console.print("  [yellow]æœªæ‰¾åˆ°ç­›é€‰æŒ‰é’®ï¼Œä½¿ç”¨é»˜è®¤æ’åº[/yellow]")
        return

    await page.evaluate("document.querySelector('div.filter').click()")
    await random_delay(0.8, 1.2)

    # ç­‰å¾… filter-panel å‡ºç°
    try:
        panel = await page.wait_for_selector("div.filter-panel", timeout=3000)
    except Exception:
        panel = None
    if not panel:
        console.print("  [yellow]ç­›é€‰é¢æ¿æœªæ‰“å¼€ï¼Œä½¿ç”¨é»˜è®¤æ’åº[/yellow]")
        return

    # åœ¨ filter-panel ä¸­ç‚¹å‡»æ’åºé€‰é¡¹å’Œæ—¥æœŸé€‰é¡¹
    result = await page.evaluate("""
        ({sortText, dateText}) => {
            const panel = document.querySelector('div.filter-panel');
            if (!panel) return { sort: false, date: false };

            let sortClicked = false;
            let dateClicked = false;

            // æ‰¾åˆ°æ‰€æœ‰å¶å­èŠ‚ç‚¹ï¼ˆæ— å­å…ƒç´ æ–‡æœ¬èŠ‚ç‚¹çš„åŒ…è£¹å…ƒç´ ï¼‰
            const allEls = panel.querySelectorAll('div, span, button, a, li');
            for (const el of allEls) {
                const text = el.innerText?.trim();
                if (!text) continue;

                // ç²¾ç¡®åŒ¹é…æ’åºé€‰é¡¹
                if (text === sortText && !sortClicked) {
                    el.click();
                    sortClicked = true;
                }
                // ç²¾ç¡®åŒ¹é…æ—¥æœŸé€‰é¡¹
                if (text === dateText && !dateClicked) {
                    el.click();
                    dateClicked = true;
                }
            }

            return { sort: sortClicked, date: dateClicked };
        }
    """, {"sortText": sort_text, "dateText": date_text})

    if result.get("sort"):
        console.print(f"  [green]âœ“ å·²é€‰æ‹©æ’åº: {sort_text}[/green]")
    else:
        console.print(f"  [yellow]æœªæ‰¾åˆ°æ’åºé€‰é¡¹ã€Œ{sort_text}ã€[/yellow]")

    if result.get("date"):
        console.print(f"  [green]âœ“ å·²é€‰æ‹©æ—¶é—´: {date_text}[/green]")
    else:
        console.print(f"  [yellow]æœªæ‰¾åˆ°æ—¶é—´é€‰é¡¹ã€Œ{date_text}ã€[/yellow]")

    # ç­‰å¾…ç­›é€‰ç”Ÿæ•ˆï¼ˆé¡µé¢ä¼šé‡æ–°åŠ è½½ç»“æœï¼‰
    await random_delay(2, 3)


async def search_keyword(page, keyword: str, sort: str = "hot"):
    """
    åœ¨å°çº¢ä¹¦æœç´¢å…³é”®è¯å¹¶æŒ‰æŒ‡å®šæ–¹å¼æ’åº
    æ’åºå’Œæ—¥æœŸç­›é€‰éƒ½åœ¨ã€Œç­›é€‰ã€é¢æ¿é‡Œï¼ˆç‚¹å‡» div.filter â†’ div.filter-panelï¼‰
    æ’åºä¾æ®: ç»¼åˆ / æœ€æ–° / æœ€å¤šç‚¹èµ / æœ€å¤šè¯„è®º / æœ€å¤šæ”¶è—
    å‘å¸ƒæ—¶é—´: ä¸é™ / ä¸€å¤©å†… / ä¸€å‘¨å†… / åŠå¹´å†…
    """
    selectors = _load_selectors()

    # æ„å»ºæœç´¢ URL
    search_url = config.XIAOHONGSHU_SEARCH.format(keyword=keyword)
    console.print(f"  [cyan]æœç´¢å…³é”®è¯: {keyword}[/cyan]")

    await navigate_to(page, search_url)
    await random_delay(*config.PAGE_LOAD_WAIT)

    # å¦‚æœæœç´¢é¡µå¼¹å‡ºç™»å½•çª—å£ï¼Œç­‰å¾…ç”¨æˆ·ç™»å½•
    logged_in = await ensure_login_on_page(page)
    if not logged_in:
        console.print("[red]ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æœç´¢[/red]")
        return False

    await random_delay(1, 2)

    # ç­›é€‰ï¼šç‚¹å‡»ã€Œç­›é€‰ã€æŒ‰é’® â†’ æ‰“å¼€ filter-panel â†’ é€‰æ‹©æ’åºå’Œæ—¥æœŸ
    await _apply_filters(page, sort)


async def scrape_note_list(page, selectors: dict, count: int) -> list[dict]:
    """
    ä»æœç´¢ç»“æœé¡µæŠ“å–ç¬”è®°åˆ—è¡¨çš„åŸºç¡€ä¿¡æ¯
    """
    notes = []
    note_selectors = selectors["search"]["note_item"].split(", ")

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        console=console,
    ) as progress:
        task = progress.add_task(f"æŠ“å–ç¬”è®°ä¸­ (ç›®æ ‡: {count} ç¯‡)...", total=None)

        scroll_attempts = 0
        max_scroll_attempts = 20

        while len(notes) < count and scroll_attempts < max_scroll_attempts:
            # æŸ¥æ‰¾é¡µé¢ä¸Šæ‰€æœ‰ç¬”è®°å…ƒç´ 
            note_elements = []
            for sel in note_selectors:
                try:
                    elements = await page.query_selector_all(sel)
                except Exception as e:
                    if "Target" in str(e) and "closed" in str(e):
                        console.print("  [red]æµè§ˆå™¨é¡µé¢å·²å…³é—­ï¼Œåœæ­¢æŠ“å–[/red]")
                        return notes
                    raise
                if elements:
                    note_elements = elements
                    break

            # æå–æ¯ç¯‡ç¬”è®°çš„åŸºç¡€ä¿¡æ¯
            for element in note_elements:
                if len(notes) >= count:
                    break

                try:
                    # è·å–ç¬”è®°é“¾æ¥
                    link_el = await element.query_selector("a")
                    href = await extract_attribute(link_el, "href") if link_el else ""

                    # è·³è¿‡å·²æŠ“å–çš„
                    if any(n.get("url") == href for n in notes):
                        continue

                    # è·å–æ ‡é¢˜
                    title_sels = selectors["search"].get("note_title", "a.title").split(", ")
                    title_el = None
                    for ts in title_sels:
                        title_el = await element.query_selector(ts)
                        if title_el:
                            break
                    if not title_el:
                        title_el = await element.query_selector("a, span")
                    title = await extract_text(title_el, "æ— æ ‡é¢˜")

                    # è·å–äº’åŠ¨æ•°æ®ï¼ˆåˆ—è¡¨é¡µå¯èƒ½åªæœ‰ç‚¹èµæ•°ï¼‰
                    like_sels = selectors["search"].get("note_like_count", ".like-wrapper .count").split(", ")
                    like_el = None
                    for ls in like_sels:
                        like_el = await element.query_selector(ls)
                        if like_el:
                            break
                    like_text = await extract_text(like_el, "0")

                    notes.append({
                        "title": title,
                        "url": href,
                        "like_count": parse_count(like_text),
                        "scraped_at": datetime.now().isoformat(),
                    })

                    progress.update(task, description=f"æŠ“å–ç¬”è®°ä¸­ ({len(notes)}/{count})...")

                except Exception as e:
                    console.print(f"  [dim]è·³è¿‡ä¸€æ¡ç¬”è®°: {e}[/dim]")
                    continue

            # æ»šåŠ¨åŠ è½½æ›´å¤š
            if len(notes) < count:
                await smooth_scroll(page, distance=500, times=2)
                await random_delay(*config.SCRAPE_DELAY)
                scroll_attempts += 1

    console.print(f"  [green]âœ“ å…±æŠ“å–åˆ° {len(notes)} ç¯‡ç¬”è®°åŸºç¡€ä¿¡æ¯[/green]")
    return notes


async def scrape_note_detail_via_popup(page, note_element, selectors: dict) -> dict:
    """
    é€šè¿‡ç‚¹å‡»æœç´¢ç»“æœä¸­çš„ç¬”è®°å¡ç‰‡å¼¹å‡ºè¯¦æƒ…å¼¹çª—ï¼ŒæŠ“å–å®Œæ•´ä¿¡æ¯ã€‚
    å°çº¢ä¹¦çš„æœç´¢é¡µæ˜¯ SPAï¼Œç‚¹å‡»ç¬”è®°ä¸ä¼šè·³è½¬é¡µé¢ï¼Œè€Œæ˜¯å¼¹å‡º overlay å¼¹çª—ã€‚

    Args:
        page: å½“å‰æœç´¢ç»“æœé¡µé¢
        note_element: ç¬”è®°å¡ç‰‡çš„ DOM å…ƒç´ å¥æŸ„
        selectors: é€‰æ‹©å™¨é…ç½®

    Returns:
        dict: ç¬”è®°è¯¦æƒ…æ•°æ®
    """
    detail_selectors = selectors["note_detail"]
    note_data = {}

    ERROR_TEXTS = ["å½“å‰ç¬”è®°æš‚æ—¶æ— æ³•æµè§ˆ", "ç¬”è®°ä¸å­˜åœ¨", "å†…å®¹å·²è¢«åˆ é™¤", "é¡µé¢ä¸å­˜åœ¨"]

    try:
        # æ»šåŠ¨åˆ°ç¬”è®°å¡ç‰‡ä½¿å…¶å¯è§
        try:
            await note_element.scroll_into_view_if_needed(timeout=3000)
            await random_delay(0.3, 0.5)
        except Exception:
            try:
                await page.evaluate("(el) => el.scrollIntoView({block: 'center'})", note_element)
                await random_delay(0.3, 0.5)
            except Exception:
                pass

        # ç‚¹å‡»ç¬”è®°å¡ç‰‡ â€” ä¼˜å…ˆç‚¹å‡» a.coverï¼ˆç»å®æµ‹æœ€å¯é ï¼‰
        cover_el = await note_element.query_selector("a.cover")
        if not cover_el:
            cover_el = await note_element.query_selector("a, .cover")
        click_target = cover_el if cover_el else note_element

        try:
            await click_target.click(timeout=5000)
        except Exception:
            try:
                await page.evaluate("(el) => el.click()", click_target)
            except Exception as click_err:
                console.print(f"    [yellow]æ— æ³•ç‚¹å‡»ç¬”è®°: {click_err}[/yellow]")
                return note_data

        await random_delay(1.5, 2.5)

        # ç­‰å¾…å¼¹çª—/è¯¦æƒ…é¡µå‡ºç° â€” ä¾æ¬¡æ£€æŸ¥å¤šç§å®¹å™¨
        popup = None
        for sel_key in ["popup_mask", "popup_container", "note_scroller"]:
            sels = detail_selectors.get(sel_key, "").split(", ")
            sels = [s for s in sels if s]
            if sels:
                popup = await wait_for_any_selector(page, sels, timeout=3000)
            if popup:
                break

        if not popup:
            console.print(f"    [yellow]å¼¹çª—/è¯¦æƒ…é¡µæœªæ‰“å¼€[/yellow]")
            return note_data

        # æ£€æµ‹æ˜¯å¦ä¸ºé”™è¯¯é¡µé¢
        try:
            page_text = await page.evaluate("() => document.body.innerText.substring(0, 500)")
            if any(err in page_text for err in ERROR_TEXTS):
                console.print(f"    [yellow]âš  è¯¥ç¬”è®°æ— æ³•æµè§ˆï¼Œè·³è¿‡[/yellow]")
                note_data["detail_status"] = "web_restricted"
                return note_data
        except Exception:
            pass

        await random_delay(0.5, 1)

        # æ•è·å¼¹çª—/è¯¦æƒ…é¡µçš„ URLï¼ˆæ˜¯ç¬”è®°çš„ç‹¬ç«‹é“¾æ¥ï¼‰
        detail_url = page.url
        if "/explore/" in detail_url:
            note_data["detail_url"] = detail_url

        # æŠ“å–æ ‡é¢˜
        title_sels = detail_selectors["title"].split(", ")
        title_el = await wait_for_any_selector(page, title_sels, timeout=3000)
        if title_el:
            note_data["title"] = await extract_text(title_el, "")

        # æŠ“å–æ­£æ–‡
        content_sels = detail_selectors["content"].split(", ")
        content_el = await wait_for_any_selector(page, content_sels, timeout=3000)
        if content_el:
            note_data["content"] = await extract_text(content_el, "")

        # æŠ“å–äº’åŠ¨æ•°æ®
        for field, sel_key in [
            ("like_count", "like_count"),
            ("collect_count", "collect_count"),
            ("comment_count", "comment_count"),
        ]:
            sels = detail_selectors[sel_key].split(", ")
            el = await wait_for_any_selector(page, sels, timeout=2000)
            if el:
                text = await extract_text(el, "0")
                note_data[field] = parse_count(text)

        # æŠ“å–æ ‡ç­¾
        tag_sels = detail_selectors["tags"].split(", ")
        tags = []
        for sel in tag_sels:
            tag_elements = await page.query_selector_all(sel)
            for tag_el in tag_elements:
                tag_text = await extract_text(tag_el)
                if tag_text:
                    tag_text = tag_text.strip()
                    if not tag_text.startswith("#"):
                        tag_text = f"#{tag_text}"
                    tags.append(tag_text)
        note_data["tags"] = list(set(tags))

        # æŠ“å–å‘å¸ƒæ—¶é—´
        time_sels = detail_selectors["publish_time"].split(", ")
        time_el = await wait_for_any_selector(page, time_sels, timeout=2000)
        if time_el:
            note_data["publish_time"] = await extract_text(time_el, "")

        # æŠ“å–ä½œè€…
        author_sels = detail_selectors["author_name"].split(", ")
        author_el = await wait_for_any_selector(page, author_sels, timeout=2000)
        if author_el:
            note_data["author"] = await extract_text(author_el, "")

        note_data["detail_status"] = "ok"

    except Exception as e:
        console.print(f"    [yellow]æŠ“å–å¼¹çª—è¯¦æƒ…å‡ºé”™: {e}[/yellow]")
        note_data["detail_status"] = "error"

    return note_data


async def _close_detail_popup(page, detail_selectors: dict):
    """å…³é—­ç¬”è®°è¯¦æƒ…å¼¹çª—ï¼Œå›åˆ°æœç´¢ç»“æœé¡µ"""
    # æ–¹æ³•1ï¼šç‚¹å‡» div.close-boxï¼ˆå®æµ‹ç¡®è®¤å­˜åœ¨ï¼‰
    close_sel = detail_selectors.get("close_button", "div.close-box")
    try:
        close_btn = await page.query_selector(close_sel)
        if close_btn and await close_btn.is_visible():
            await close_btn.click()
            await random_delay(0.5, 1)
            # éªŒè¯å¼¹çª—æ˜¯å¦å…³é—­
            mask = await page.query_selector(".note-detail-mask")
            if not mask:
                return
    except Exception:
        pass

    # æ–¹æ³•2ï¼šæŒ‰ Escapeï¼ˆå®æµ‹ç¡®è®¤æœ‰æ•ˆï¼Œä¸”ä¿ç•™æœç´¢ç»“æœ DOMï¼‰
    try:
        await page.keyboard.press("Escape")
        await random_delay(0.5, 1)
    except Exception:
        pass


def generate_analysis_report(notes: list[dict], keyword: str) -> str:
    """
    æ ¹æ®æŠ“å–çš„ç¬”è®°æ•°æ®ï¼Œç”Ÿæˆ Markdown åˆ†ææŠ¥å‘Š
    """
    report_lines = [
        f"# å°çº¢ä¹¦çƒ­é—¨ç¬”è®°åˆ†ææŠ¥å‘Š",
        f"",
        f"- **æœç´¢å…³é”®è¯**: {keyword}",
        f"- **åˆ†æç¬”è®°æ•°**: {len(notes)} ç¯‡",
        f"- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"",
        f"---",
        f"",
    ]

    # ---- 1. äº’åŠ¨æ•°æ®æ’è¡Œ ----
    report_lines.append("## ğŸ“Š äº’åŠ¨æ•°æ® Top 10")
    report_lines.append("")

    sorted_by_like = sorted(notes, key=lambda x: x.get("like_count", 0), reverse=True)[:10]
    report_lines.append("| æ’å | æ ‡é¢˜ | ğŸ‘ ç‚¹èµ | â­ æ”¶è— | ğŸ’¬ è¯„è®º |")
    report_lines.append("|------|------|---------|---------|---------|")
    for i, note in enumerate(sorted_by_like, 1):
        title = truncate_text(note.get("title", "æ— æ ‡é¢˜"), 30)
        like = note.get("like_count", 0)
        collect = note.get("collect_count", 0)
        comment = note.get("comment_count", 0)
        report_lines.append(f"| {i} | {title} | {like} | {collect} | {comment} |")
    report_lines.append("")

    # ---- 2. é«˜é¢‘å…³é”®è¯ ----
    report_lines.append("## ğŸ”‘ é«˜é¢‘å…³é”®è¯ Top 20")
    report_lines.append("")

    all_text = " ".join(
        (note.get("title", "") + " " + note.get("content", ""))
        for note in notes
    )
    # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰æ ‡ç‚¹å’Œç©ºæ ¼åˆ†å‰²ï¼Œè¿‡æ»¤çŸ­è¯ï¼‰
    words = re.findall(r'[\u4e00-\u9fff]{2,4}', all_text)
    # è¿‡æ»¤å¸¸è§åœç”¨è¯
    stopwords = {"ä»€ä¹ˆ", "æ€ä¹ˆ", "è¿™ä¸ª", "é‚£ä¸ª", "ä¸€ä¸ª", "å¯ä»¥", "å°±æ˜¯", "çœŸçš„",
                 "å¤§å®¶", "è‡ªå·±", "ä¸æ˜¯", "æ²¡æœ‰", "å·²ç»", "è¿˜æ˜¯", "æˆ‘ä»¬", "ä»–ä»¬",
                 "çŸ¥é“", "è§‰å¾—", "å› ä¸º", "æ‰€ä»¥", "ä½†æ˜¯", "è€Œä¸”", "æˆ–è€…", "å¦‚æœ"}
    filtered = [w for w in words if w not in stopwords]
    word_freq = Counter(filtered).most_common(20)

    report_lines.append("| æ’å | å…³é”®è¯ | å‡ºç°æ¬¡æ•° |")
    report_lines.append("|------|--------|----------|")
    for i, (word, freq) in enumerate(word_freq, 1):
        report_lines.append(f"| {i} | {word} | {freq} |")
    report_lines.append("")

    # ---- 3. æ ‡é¢˜æ¨¡å¼åˆ†æ ----
    report_lines.append("## ğŸ“ æ ‡é¢˜æ¨¡å¼åˆ†æ")
    report_lines.append("")

    titles = [note.get("title", "") for note in notes if note.get("title")]
    avg_title_len = sum(len(t) for t in titles) / len(titles) if titles else 0
    report_lines.append(f"- **å¹³å‡æ ‡é¢˜é•¿åº¦**: {avg_title_len:.0f} å­—")

    # æ ‡é¢˜ä¸­å¸¸è§å¥å¼
    question_titles = sum(1 for t in titles if "?" in t or "ï¼Ÿ" in t or "å—" in t)
    number_titles = sum(1 for t in titles if re.search(r'\d+', t))
    emoji_titles = sum(1 for t in titles if re.search(r'[^\w\s\u4e00-\u9fff]', t))
    report_lines.append(f"- **ç–‘é—®å¥æ ‡é¢˜**: {question_titles} ç¯‡ ({question_titles/len(titles)*100:.0f}%)")
    report_lines.append(f"- **å«æ•°å­—æ ‡é¢˜**: {number_titles} ç¯‡ ({number_titles/len(titles)*100:.0f}%)")
    report_lines.append(f"- **å« Emoji æ ‡é¢˜**: {emoji_titles} ç¯‡ ({emoji_titles/len(titles)*100:.0f}%)")
    report_lines.append("")

    # ---- 4. æ ‡ç­¾ç­–ç•¥ ----
    report_lines.append("## ğŸ·ï¸ æ ‡ç­¾ä½¿ç”¨ç­–ç•¥")
    report_lines.append("")

    all_tags = []
    for note in notes:
        all_tags.extend(note.get("tags", []))
    tag_freq = Counter(all_tags).most_common(15)

    if tag_freq:
        avg_tags = sum(len(note.get("tags", [])) for note in notes) / len(notes) if notes else 0
        report_lines.append(f"- **å¹³å‡æ¯ç¯‡æ ‡ç­¾æ•°**: {avg_tags:.1f}")
        report_lines.append(f"- **æœ€çƒ­é—¨æ ‡ç­¾**:")
        report_lines.append("")
        report_lines.append("| æ ‡ç­¾ | ä½¿ç”¨æ¬¡æ•° |")
        report_lines.append("|------|----------|")
        for tag, freq in tag_freq:
            report_lines.append(f"| {tag} | {freq} |")
    else:
        report_lines.append("- æœªæŠ“å–åˆ°æ ‡ç­¾æ•°æ®")
    report_lines.append("")

    # ---- 5. å†…å®¹é•¿åº¦åˆ†æ ----
    report_lines.append("## ğŸ“ å†…å®¹é•¿åº¦ä¸äº’åŠ¨ç‡å…³ç³»")
    report_lines.append("")

    notes_with_content = [n for n in notes if n.get("content")]
    if notes_with_content:
        short = [n for n in notes_with_content if len(n["content"]) < 200]
        medium = [n for n in notes_with_content if 200 <= len(n["content"]) < 500]
        long = [n for n in notes_with_content if len(n["content"]) >= 500]

        def avg_engagement(group):
            if not group:
                return 0
            return sum(n.get("like_count", 0) + n.get("collect_count", 0) + n.get("comment_count", 0) for n in group) / len(group)

        report_lines.append("| å†…å®¹é•¿åº¦ | ç¬”è®°æ•° | å¹³å‡äº’åŠ¨é‡ |")
        report_lines.append("|----------|--------|------------|")
        report_lines.append(f"| çŸ­ (<200å­—) | {len(short)} | {avg_engagement(short):.0f} |")
        report_lines.append(f"| ä¸­ (200-500å­—) | {len(medium)} | {avg_engagement(medium):.0f} |")
        report_lines.append(f"| é•¿ (>500å­—) | {len(long)} | {avg_engagement(long):.0f} |")
    else:
        report_lines.append("- æœªæŠ“å–åˆ°æ­£æ–‡å†…å®¹ï¼Œæ— æ³•åˆ†æ")
    report_lines.append("")

    # ---- 6. åˆ›ä½œå»ºè®® ----
    report_lines.append("## ğŸ’¡ åˆ›ä½œå»ºè®®")
    report_lines.append("")

    if word_freq:
        top_keywords = "ã€".join(w for w, _ in word_freq[:5])
        report_lines.append(f"1. **å…³é”®è¯çƒ­ç‚¹**: å›´ç»•ã€Œ{top_keywords}ã€ç­‰é«˜é¢‘è¯åˆ›ä½œ")
    report_lines.append(f"2. **æ ‡é¢˜é•¿åº¦**: å»ºè®®æ§åˆ¶åœ¨ {max(10, int(avg_title_len - 5))}-{int(avg_title_len + 5)} å­—")
    if number_titles > len(titles) * 0.3:
        report_lines.append("3. **æ•°å­—æ ‡é¢˜**: è¯¥é¢†åŸŸå«æ•°å­—çš„æ ‡é¢˜æ•ˆæœå¥½ï¼Œå»ºè®®ä½¿ç”¨å…·ä½“æ•°æ®")
    if emoji_titles > len(titles) * 0.3:
        report_lines.append("4. **Emoji ä½¿ç”¨**: è¯¥é¢†åŸŸ Emoji ä½¿ç”¨ç‡é«˜ï¼Œå»ºè®®é€‚å½“æ·»åŠ ")
    if tag_freq:
        top_tags = "ã€".join(t for t, _ in tag_freq[:5])
        report_lines.append(f"5. **æ¨èæ ‡ç­¾**: {top_tags}")
    report_lines.append("")

    return "\n".join(report_lines)


async def analyze(keyword: str, count: int = 20, sort: str = "hot", output: str = None):
    """
    ä¸»åˆ†ææµç¨‹
    """
    config.ensure_dirs()
    selectors = _load_selectors()

    console.print(Panel(
        f"ğŸ” å°çº¢ä¹¦çƒ­é—¨ç¬”è®°åˆ†æ\n"
        f"   å…³é”®è¯: {keyword}\n"
        f"   æ•°é‡: {count} ç¯‡\n"
        f"   æ’åº: {'æœ€çƒ­' if sort == 'hot' else 'æœ€æ–°'}",
        style="bold cyan",
    ))

    # å¯åŠ¨æµè§ˆå™¨
    context, page = await launch_browser()

    try:
        # ç™»å½•ï¼ˆç»Ÿä¸€ç”± browser_helper å¤„ç†ï¼‰
        logged_in = await ensure_login(page)
        if not logged_in:
            console.print("[red]æœªèƒ½ç™»å½•ï¼Œé€€å‡º[/red]")
            return

        # æœç´¢
        search_result = await search_keyword(page, keyword, sort)
        if search_result is False:
            console.print("[red]æœç´¢å¤±è´¥ï¼ˆå¯èƒ½æœªç™»å½•ï¼‰ï¼Œé€€å‡º[/red]")
            return

        # æŠ“å–ç¬”è®°åˆ—è¡¨
        try:
            notes = await scrape_note_list(page, selectors, count)
        except Exception as e:
            if "Target" in str(e) and "closed" in str(e):
                console.print("[red]æµè§ˆå™¨æ„å¤–å…³é—­ï¼Œè¯·é‡æ–°è¿è¡Œ[/red]")
                return
            raise

        if not notes:
            console.print("[red]æœªæŠ“å–åˆ°ä»»ä½•ç¬”è®°ï¼Œè¯·æ£€æŸ¥æœç´¢å…³é”®è¯æˆ–ç½‘ç»œ[/red]")
            return

        # é€ç¯‡ç‚¹å‡»ç¬”è®°å¼¹çª—æŠ“å–è¯¦æƒ…
        console.print(f"\n[cyan]æ­£åœ¨é€ç¯‡ç‚¹å‡»ç¬”è®°æŠ“å–è¯¦æƒ… ({len(notes)} ç¯‡)...[/cyan]")
        search_url = config.XIAOHONGSHU_SEARCH.format(keyword=keyword)
        detail_selectors = selectors["note_detail"]
        note_item_sels = selectors["search"]["note_item"].split(", ")

        for i, note in enumerate(notes):
            note_url = note.get('url', '')
            # æ„å»ºå®Œæ•´ç¬”è®° URL ç”¨äºæ˜¾ç¤º
            if note_url and not note_url.startswith('http'):
                full_note_url = f"https://www.xiaohongshu.com{note_url}"
            else:
                full_note_url = note_url
            console.print(f"  [{i + 1}/{len(notes)}] {truncate_text(note.get('title', ''), 40)}")
            if full_note_url:
                console.print(f"    [dim]URL: {full_note_url}[/dim]")

            try:
                # ç¡®ä¿åœ¨æœç´¢é¡µä¸Šï¼ˆæ¯æ¬¡å¾ªç¯éƒ½å›åˆ°æœç´¢é¡µï¼‰
                current_url = page.url
                if "search_result" not in current_url:
                    await navigate_to(page, search_url)
                    await random_delay(1, 2)

                # é‡æ–°æŸ¥æ‰¾ç¬”è®°å…ƒç´ ï¼ˆæ¯æ¬¡éƒ½æŸ¥ï¼Œå› ä¸º DOM å¯èƒ½é‡å»ºäº†ï¼‰
                note_elements = []
                for sel in note_item_sels:
                    note_elements = await page.query_selector_all(sel)
                    if note_elements:
                        break

                # æ‰¾åˆ°ä¸å½“å‰ note å¯¹åº”çš„å…ƒç´ 
                target_el = None
                note_url = note.get("url", "")

                # æ–¹æ³•1ï¼šé€šè¿‡ URL åŒ¹é…
                if note_url:
                    for el in note_elements:
                        link_el = await el.query_selector("a")
                        if link_el:
                            href = await extract_attribute(link_el, "href")
                            if href and note_url in href:
                                target_el = el
                                break

                # æ–¹æ³•2ï¼šå¦‚æœ URL åŒ¹é…å¤±è´¥ï¼Œé€šè¿‡æ ‡é¢˜åŒ¹é…
                if not target_el and note.get("title"):
                    for el in note_elements:
                        el_text = await extract_text(el, "")
                        if note.get("title", "NOMATCH") in el_text:
                            target_el = el
                            break

                # æ–¹æ³•3ï¼šæŒ‰ä½ç½®ï¼ˆæœ€åæ‰‹æ®µï¼‰
                if not target_el and i < len(note_elements):
                    target_el = note_elements[i]

                if not target_el:
                    console.print(f"    [yellow]æœªæ‰¾åˆ°å¯¹åº”å…ƒç´ ï¼Œè·³è¿‡[/yellow]")
                    continue

                # ç‚¹å‡»å¹¶æŠ“å–è¯¦æƒ…
                detail_data = await scrape_note_detail_via_popup(page, target_el, selectors)

                # åˆå¹¶å¼¹çª—æ•°æ®åˆ°åˆ—è¡¨æ•°æ®
                if detail_data:
                    for key, value in detail_data.items():
                        if value:
                            note[key] = value

                # å…³é—­å¼¹çª— / å›åˆ°æœç´¢ç»“æœé¡µ
                await _close_detail_popup(page, detail_selectors)

            except Exception as e:
                if "Target" in str(e) and "closed" in str(e):
                    console.print("  [red]æµè§ˆå™¨å·²å…³é—­ï¼Œåœæ­¢è¯¦æƒ…æŠ“å–[/red]")
                    break
                console.print(f"    [yellow]è¯¦æƒ…æŠ“å–å‡ºé”™: {e}[/yellow]")

            await random_delay(*config.SCRAPE_DELAY)

        # ä¿å­˜åŸå§‹æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = config.OUTPUT_DIR / f"notes_{keyword}_{timestamp}.json"
        save_to_json(notes, json_path)

        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = generate_analysis_report(notes, keyword)
        report_path = Path(output) if output else config.OUTPUT_DIR / f"report_{keyword}_{timestamp}.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        console.print(f"\n[green]âœ“ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}[/green]")

        # åœ¨ç»ˆç«¯å±•ç¤ºæ‘˜è¦
        _print_summary(notes, keyword)

    finally:
        await close_browser(context)


def _print_summary(notes: list[dict], keyword: str):
    """åœ¨ç»ˆç«¯æ‰“å°åˆ†ææ‘˜è¦"""
    table = Table(title=f"ğŸ”¥ ã€Œ{keyword}ã€çƒ­é—¨ç¬”è®° Top 5", show_lines=True)
    table.add_column("æ ‡é¢˜", style="cyan", max_width=35)
    table.add_column("ğŸ‘", justify="right", style="green")
    table.add_column("â­", justify="right", style="yellow")
    table.add_column("ğŸ’¬", justify="right", style="blue")
    table.add_column("URL", style="dim", max_width=40)

    # æŒ‰ URL å»é‡
    seen_urls = set()
    unique_notes = []
    for note in notes:
        url = note.get("url", "")
        if url and url in seen_urls:
            continue
        seen_urls.add(url)
        unique_notes.append(note)

    sorted_notes = sorted(unique_notes, key=lambda x: x.get("like_count", 0), reverse=True)[:5]
    for note in sorted_notes:
        note_url = note.get("url", "")
        if note_url and not note_url.startswith("http"):
            note_url = f"https://www.xiaohongshu.com{note_url}"
        # æˆªæ–­ URL æ˜¾ç¤ºï¼ˆå»æ‰ query paramsï¼‰
        display_url = note_url.split("?")[0] if note_url else ""
        table.add_row(
            truncate_text(note.get("title", ""), 35),
            str(note.get("like_count", 0)),
            str(note.get("collect_count", 0)),
            str(note.get("comment_count", 0)),
            display_url,
        )

    console.print()
    console.print(table)


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦çƒ­é—¨ç¬”è®°åˆ†æå·¥å…·")
    parser.add_argument("--keyword", "-k", required=True, help="æœç´¢å…³é”®è¯")
    parser.add_argument("--count", "-c", type=int, default=config.DEFAULT_ARTICLE_COUNT,
                        help=f"æŠ“å–ç¬”è®°æ•°é‡ (é»˜è®¤: {config.DEFAULT_ARTICLE_COUNT})")
    parser.add_argument("--sort", "-s", choices=["hot", "new"], default="hot",
                        help="æ’åºæ–¹å¼: hot(æœ€çƒ­), new(æœ€æ–°) (é»˜è®¤: hot)")
    parser.add_argument("--output", "-o", help="åˆ†ææŠ¥å‘Šè¾“å‡ºè·¯å¾„ (é»˜è®¤: output/report_<keyword>_<time>.md)")

    args = parser.parse_args()

    # é™åˆ¶æ•°é‡
    count = min(args.count, config.MAX_ARTICLE_COUNT)

    asyncio.run(analyze(
        keyword=args.keyword,
        count=count,
        sort=args.sort,
        output=args.output,
    ))


if __name__ == "__main__":
    main()
