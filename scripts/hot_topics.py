"""
çƒ­é—¨è¯é¢˜æ’è¡Œæ¦œè„šæœ¬
æŠ“å–å°çº¢ä¹¦è¿‘æœŸçƒ­é—¨è¯é¢˜/è¶‹åŠ¿ï¼Œç”Ÿæˆ Top 10/20 æ’è¡Œæ¦œ
"""

import argparse
import asyncio
import json
from datetime import datetime
from pathlib import Path

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

import config
from browser_helper import launch_browser, ensure_login, close_browser, navigate_to
from utils import (
    random_delay, extract_text, extract_attribute,
    parse_count, save_to_json, smooth_scroll, wait_for_any_selector,
)

console = Console()


def _load_selectors() -> dict:
    with open(config.SELECTORS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


async def scrape_explore_topics(page, selectors: dict, count: int) -> list[dict]:
    """
    ä»å°çº¢ä¹¦å‘ç°/æ¢ç´¢é¡µé¢æŠ“å–çƒ­é—¨è¯é¢˜
    é€šè¿‡åˆ†æé¦–é¡µæ¨èæµä¸­çš„é«˜é¢‘è¯é¢˜æ¥è·å–è¶‹åŠ¿
    """
    topics = []
    topic_sels = selectors.get("explore", {})

    console.print("  [cyan]æ­£åœ¨æŠ“å–æ¢ç´¢é¡µçƒ­é—¨å†…å®¹...[/cyan]")

    await navigate_to(page, config.XIAOHONGSHU_EXPLORE)
    await random_delay(*config.PAGE_LOAD_WAIT)

    # å°è¯•ä»æ¢ç´¢é¡µçš„è¯é¢˜æ¨èåŒºåŸŸæŠ“å–
    topic_card_sels = topic_sels.get("topic_card", ".topic-card, .channel-item, .category-item").split(", ")
    topic_name_sels = topic_sels.get("topic_name", ".topic-name, .channel-name, .title, span").split(", ")
    topic_count_sels = topic_sels.get("topic_view_count", ".view-count, .count, .desc").split(", ")

    # å…ˆå°è¯•ç›´æ¥è·å–è¯é¢˜å¡ç‰‡
    for sel in topic_card_sels:
        cards = await page.query_selector_all(sel)
        if cards:
            for card in cards:
                if len(topics) >= count:
                    break
                try:
                    name_el = None
                    for ns in topic_name_sels:
                        name_el = await card.query_selector(ns)
                        if name_el:
                            break
                    name = await extract_text(name_el, "") if name_el else ""
                    if not name:
                        continue

                    count_el = None
                    for cs in topic_count_sels:
                        count_el = await card.query_selector(cs)
                        if count_el:
                            break
                    view_text = await extract_text(count_el, "0") if count_el else "0"

                    link_el = await card.query_selector("a")
                    href = await extract_attribute(link_el, "href", "") if link_el else ""

                    topics.append({
                        "name": name.strip().lstrip("#"),
                        "view_count": parse_count(view_text),
                        "url": href,
                        "source": "explore_page",
                    })
                except Exception:
                    continue
            if topics:
                break

    return topics


async def scrape_trending_from_search(page, selectors: dict, count: int) -> list[dict]:
    """
    é€šè¿‡æœç´¢é¡µé¢çš„çƒ­æœè¯/æ¨èè¯æ¥è·å–çƒ­é—¨è¯é¢˜
    å°çº¢ä¹¦æœç´¢æ¡†ç‚¹å‡»åé€šå¸¸ä¼šå±•ç¤ºçƒ­æœæ¦œ
    """
    topics = []
    search_sels = selectors.get("search", {})
    trending_sels = selectors.get("trending", {})

    console.print("  [cyan]æ­£åœ¨è·å–æœç´¢çƒ­è¯...[/cyan]")

    await navigate_to(page, config.XIAOHONGSHU_HOME)
    await random_delay(*config.PAGE_LOAD_WAIT)

    # ç‚¹å‡»æœç´¢æ¡†ï¼Œè§¦å‘çƒ­æœå±•ç¤º
    search_input_sels = search_sels.get("search_input", "#search-input").split(", ")
    search_input = await wait_for_any_selector(page, search_input_sels, timeout=8000)

    if search_input:
        await search_input.click()
        await random_delay(1, 2)

        # æŠ“å–çƒ­æœåˆ—è¡¨
        hot_item_sels = trending_sels.get(
            "hot_search_item",
            ".trending-item, .hot-item, .search-trending-item, .hot-list-item, .hot-word"
        ).split(", ")
        hot_name_sels = trending_sels.get(
            "hot_search_name",
            ".title, .name, .word, span, a"
        ).split(", ")
        hot_rank_sels = trending_sels.get(
            "hot_search_rank",
            ".rank, .index, .num"
        ).split(", ")
        hot_heat_sels = trending_sels.get(
            "hot_search_heat",
            ".hot-score, .heat, .score, .count"
        ).split(", ")

        for sel in hot_item_sels:
            items = await page.query_selector_all(sel)
            if items:
                for item in items:
                    if len(topics) >= count:
                        break
                    try:
                        # è·å–è¯é¢˜å
                        name_el = None
                        for ns in hot_name_sels:
                            name_el = await item.query_selector(ns)
                            if name_el:
                                break
                        name = await extract_text(name_el, "") if name_el else ""
                        if not name or len(name) < 2:
                            continue

                        # è·å–æ’å
                        rank_el = None
                        for rs in hot_rank_sels:
                            rank_el = await item.query_selector(rs)
                            if rank_el:
                                break
                        rank_text = await extract_text(rank_el, "") if rank_el else ""

                        # è·å–çƒ­åº¦
                        heat_el = None
                        for hs in hot_heat_sels:
                            heat_el = await item.query_selector(hs)
                            if heat_el:
                                break
                        heat_text = await extract_text(heat_el, "0") if heat_el else "0"

                        topics.append({
                            "name": name.strip(),
                            "rank": rank_text,
                            "heat": parse_count(heat_text),
                            "source": "search_trending",
                        })
                    except Exception:
                        continue
                if topics:
                    break

    return topics


async def scrape_trending_from_feed(page, count: int) -> list[dict]:
    """
    å…œåº•ç­–ç•¥ï¼šä»é¦–é¡µä¿¡æ¯æµä¸­ç»Ÿè®¡é«˜é¢‘è¯é¢˜æ ‡ç­¾
    å³ä½¿æ²¡æœ‰å®˜æ–¹çƒ­æœå…¥å£ï¼Œä¹Ÿèƒ½é€šè¿‡åˆ†æä¿¡æ¯æµå¾—åˆ°è¶‹åŠ¿
    """
    console.print("  [cyan]æ­£åœ¨åˆ†æé¦–é¡µä¿¡æ¯æµä¸­çš„çƒ­é—¨è¯é¢˜...[/cyan]")

    await navigate_to(page, config.XIAOHONGSHU_HOME)
    await random_delay(*config.PAGE_LOAD_WAIT)

    tag_counter: dict[str, int] = {}

    # å¤šæ¬¡æ»šåŠ¨é‡‡é›†
    for scroll_round in range(8):
        # æ”¶é›†é¡µé¢ä¸­æ‰€æœ‰ hashtag é“¾æ¥
        tag_elements = await page.query_selector_all(
            "a[href*='/page/topics/'], .hashtag, .tag-item, a[href*='keyword=']"
        )

        for el in tag_elements:
            text = await extract_text(el, "")
            text = text.strip().lstrip("#")
            if text and len(text) >= 2 and len(text) <= 20:
                tag_counter[text] = tag_counter.get(text, 0) + 1

        # ä¹Ÿä»ç¬”è®°æ ‡é¢˜ä¸­æå–è¯é¢˜æ ‡ç­¾ (#xxx)
        all_text_els = await page.query_selector_all(
            ".note-item .title, .note-item span, .note-item .desc"
        )
        for el in all_text_els:
            text = await extract_text(el, "")
            import re
            tags_in_text = re.findall(r'#([\u4e00-\u9fffA-Za-z0-9]{2,15})', text)
            for tag in tags_in_text:
                tag_counter[tag] = tag_counter.get(tag, 0) + 1

        await smooth_scroll(page, distance=600, times=2)
        await random_delay(*config.SCROLL_DELAY)

        console.print(f"    [dim]ç¬¬ {scroll_round + 1}/8 è½®æ‰«æï¼Œå·²å‘ç° {len(tag_counter)} ä¸ªè¯é¢˜[/dim]")

    # æŒ‰å‡ºç°é¢‘æ¬¡æ’åº
    sorted_tags = sorted(tag_counter.items(), key=lambda x: x[1], reverse=True)[:count]

    topics = []
    for rank, (name, freq) in enumerate(sorted_tags, 1):
        topics.append({
            "name": name,
            "frequency": freq,
            "heat": freq * 100,  # ç”¨é¢‘æ¬¡ä¼°ç®—çƒ­åº¦
            "source": "feed_analysis",
        })

    return topics


def generate_trending_report(topics: list[dict], count: int) -> str:
    """ç”Ÿæˆçƒ­é—¨è¯é¢˜æ’è¡Œæ¦œ Markdown æŠ¥å‘Š"""
    lines = [
        f"# ğŸ”¥ å°çº¢ä¹¦çƒ­é—¨è¯é¢˜æ’è¡Œæ¦œ",
        f"",
        f"- **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"- **è¯é¢˜æ•°é‡**: {len(topics)}",
        f"",
        f"---",
        f"",
    ]

    # æ’è¡Œè¡¨æ ¼
    lines.append("## æ’è¡Œæ¦œ")
    lines.append("")
    lines.append("| æ’å | è¯é¢˜ | çƒ­åº¦ | æ•°æ®æ¥æº |")
    lines.append("|------|------|------|----------|")

    for i, topic in enumerate(topics[:count], 1):
        name = topic["name"]
        heat = topic.get("heat", topic.get("frequency", 0))
        source_map = {
            "search_trending": "ğŸ” æœç´¢çƒ­æœ",
            "explore_page": "ğŸŒŸ æ¢ç´¢æ¨è",
            "feed_analysis": "ğŸ“Š ä¿¡æ¯æµåˆ†æ",
        }
        source = source_map.get(topic.get("source", ""), "æœªçŸ¥")

        # å‰ä¸‰ååŠ ç«ç„° emoji
        rank_display = f"ğŸ¥‡" if i == 1 else f"ğŸ¥ˆ" if i == 2 else f"ğŸ¥‰" if i == 3 else f"{i}"

        lines.append(f"| {rank_display} | #{name} | {heat:,} | {source} |")

    lines.append("")

    # åˆ›ä½œå»ºè®®
    lines.append("## ğŸ’¡ è¹­çƒ­ç‚¹å»ºè®®")
    lines.append("")
    if len(topics) >= 3:
        top3 = [t["name"] for t in topics[:3]]
        lines.append(f"å½“å‰æœ€çƒ­è¯é¢˜æ˜¯ **#{top3[0]}**ã€**#{top3[1]}**ã€**#{top3[2]}**ã€‚")
        lines.append("")
        lines.append("å‚è€ƒæ–¹å‘ï¼š")
        lines.append(f"- å›´ç»•ã€Œ{top3[0]}ã€åˆ†äº«ä½ çš„çœŸå®ä½“éªŒæˆ–çœ‹æ³•")
        lines.append(f"- æŠŠã€Œ{top3[1]}ã€å’Œä½ çš„é¢†åŸŸåšäº¤å‰ï¼Œæ‰¾åˆ°ç‹¬ç‰¹åˆ‡å…¥ç‚¹")
        lines.append(f"- ã€Œ{top3[2]}ã€é€‚åˆå†™è§‚ç‚¹ç±»æˆ–æ•…äº‹ç±»ç¬”è®°")
    lines.append("")

    return "\n".join(lines)


async def get_trending(count: int = 20, output: str = None):
    """
    ä¸»æµç¨‹ï¼šè·å–çƒ­é—¨è¯é¢˜æ’è¡Œæ¦œ
    ä½¿ç”¨ä¸‰çº§ç­–ç•¥ï¼šæœç´¢çƒ­æœ â†’ æ¢ç´¢é¡µæ¨è â†’ ä¿¡æ¯æµåˆ†æ
    """
    config.ensure_dirs()
    selectors = _load_selectors()

    console.print(Panel(
        f"ğŸ”¥ å°çº¢ä¹¦çƒ­é—¨è¯é¢˜æ’è¡Œæ¦œ\n"
        f"   ç›®æ ‡æ•°é‡: Top {count}",
        style="bold magenta",
    ))

    context, page = await launch_browser()

    try:
        # æ£€æŸ¥ç™»å½•
        is_logged_in = await check_login_status(page)
        if not is_logged_in:
            success = await wait_for_login(page)
            if not success:
                console.print("[red]æœªèƒ½ç™»å½•ï¼Œé€€å‡º[/red]")
                return

        all_topics = []

        # ç­–ç•¥ 1ï¼šæœç´¢çƒ­æœ
        console.print("\n[bold]ğŸ“ ç­–ç•¥ 1: è·å–æœç´¢çƒ­æœ[/bold]")
        trending_topics = await scrape_trending_from_search(page, selectors, count)
        if trending_topics:
            console.print(f"  [green]âœ“ ä»æœç´¢çƒ­æœè·å–äº† {len(trending_topics)} ä¸ªè¯é¢˜[/green]")
            all_topics.extend(trending_topics)
        else:
            console.print("  [yellow]æœªèƒ½ä»æœç´¢çƒ­æœè·å–è¯é¢˜[/yellow]")

        # ç­–ç•¥ 2ï¼šæ¢ç´¢é¡µæ¨è
        if len(all_topics) < count:
            console.print("\n[bold]ğŸ“ ç­–ç•¥ 2: è·å–æ¢ç´¢é¡µæ¨èè¯é¢˜[/bold]")
            explore_topics = await scrape_explore_topics(page, selectors, count - len(all_topics))
            if explore_topics:
                console.print(f"  [green]âœ“ ä»æ¢ç´¢é¡µè·å–äº† {len(explore_topics)} ä¸ªè¯é¢˜[/green]")
                all_topics.extend(explore_topics)
            else:
                console.print("  [yellow]æœªèƒ½ä»æ¢ç´¢é¡µè·å–è¯é¢˜[/yellow]")

        # ç­–ç•¥ 3ï¼šä¿¡æ¯æµåˆ†æï¼ˆå…œåº•ï¼‰
        if len(all_topics) < count:
            console.print("\n[bold]ğŸ“ ç­–ç•¥ 3: åˆ†æé¦–é¡µä¿¡æ¯æµçƒ­é—¨æ ‡ç­¾[/bold]")
            feed_topics = await scrape_trending_from_feed(page, count - len(all_topics))
            if feed_topics:
                console.print(f"  [green]âœ“ ä»ä¿¡æ¯æµè·å–äº† {len(feed_topics)} ä¸ªè¯é¢˜[/green]")
                all_topics.extend(feed_topics)

        if not all_topics:
            console.print("\n[red]âŒ æœªèƒ½è·å–åˆ°ä»»ä½•çƒ­é—¨è¯é¢˜[/red]")
            return

        # å»é‡ï¼ˆæŒ‰è¯é¢˜åï¼‰
        seen = set()
        unique_topics = []
        for t in all_topics:
            name = t["name"]
            if name not in seen:
                seen.add(name)
                unique_topics.append(t)
        all_topics = unique_topics[:count]

        # ä¿å­˜åŸå§‹æ•°æ®
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_path = config.OUTPUT_DIR / f"hot_topics_{timestamp}.json"
        save_to_json(all_topics, json_path)

        # ç”ŸæˆæŠ¥å‘Š
        report = generate_trending_report(all_topics, count)
        report_path = Path(output) if output else config.OUTPUT_DIR / f"hot_topics_{timestamp}.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        console.print(f"\n[green]âœ“ æŠ¥å‘Šå·²ä¿å­˜: {report_path}[/green]")

        # ç»ˆç«¯å±•ç¤ºæ’è¡Œæ¦œ
        _print_ranking(all_topics, count)

    finally:
        await close_browser(context)


def _print_ranking(topics: list[dict], count: int):
    """åœ¨ç»ˆç«¯æ‰“å°æ’è¡Œæ¦œ"""
    table = Table(title="ğŸ”¥ å°çº¢ä¹¦çƒ­é—¨è¯é¢˜ Top " + str(min(count, len(topics))), show_lines=True)
    table.add_column("æ’å", justify="center", style="bold", width=6)
    table.add_column("è¯é¢˜", style="cyan", max_width=30)
    table.add_column("çƒ­åº¦", justify="right", style="magenta")
    table.add_column("æ¥æº", style="dim", max_width=15)

    source_map = {
        "search_trending": "æœç´¢çƒ­æœ",
        "explore_page": "æ¢ç´¢æ¨è",
        "feed_analysis": "ä¿¡æ¯æµåˆ†æ",
    }

    for i, topic in enumerate(topics[:count], 1):
        rank = "ğŸ¥‡" if i == 1 else "ğŸ¥ˆ" if i == 2 else "ğŸ¥‰" if i == 3 else str(i)
        heat = topic.get("heat", topic.get("frequency", 0))
        source = source_map.get(topic.get("source", ""), "æœªçŸ¥")
        table.add_row(rank, f"#{topic['name']}", f"{heat:,}", source)

    console.print()
    console.print(table)


# ============================================================
# CLI å…¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="å°çº¢ä¹¦çƒ­é—¨è¯é¢˜æ’è¡Œæ¦œ")
    parser.add_argument("--count", "-c", type=int, default=20,
                        help="æ’è¡Œæ¦œæ•°é‡ (é»˜è®¤: 20, å¯é€‰ 10/20)")
    parser.add_argument("--output", "-o", help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„")

    args = parser.parse_args()
    count = min(max(args.count, 5), 50)  # é™åˆ¶ 5-50

    asyncio.run(get_trending(count=count, output=args.output))


if __name__ == "__main__":
    main()
